"""
Comprehensive testing module for Discord bot systems.
Validates database operations, AI integrations, and core functionality.
"""

import asyncio
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple
import discord


class BotTestSuite:
    """
    Comprehensive test suite for validating bot functionality.
    Tests are organized by category and report pass/fail status.
    """

    def __init__(self, bot, guild_id: int, guild_name: str):
        """
        Initialize test suite.

        Args:
            bot: Discord bot instance
            guild_id: Guild ID to test against
            guild_name: Guild name for database lookup
        """
        self.bot = bot
        self.guild_id = guild_id
        self.guild_name = guild_name
        self.db_manager = bot.get_server_db(guild_id, guild_name)
        self.results = []

    def _log_test(self, category: str, test_name: str, passed: bool, details: str = ""):
        """Log a test result."""
        status = "PASS" if passed else "FAIL"
        emoji = "✅" if passed else "❌"
        self.results.append({
            "category": category,
            "test": test_name,
            "status": status,
            "emoji": emoji,
            "details": details,
            "passed": passed
        })

    async def run_all_tests(self) -> Dict:
        """
        Run all test categories.

        Returns:
            Dictionary with test results and summary
        """
        print(f"\n{'='*60}")
        print(f"Starting Bot Test Suite for Guild: {self.guild_name}")
        print(f"{'='*60}\n")

        # Run all test categories
        await self.test_database_connection()
        await self.test_database_tables()
        await self.test_bot_identity()
        await self.test_relationship_metrics()
        await self.test_long_term_memory()
        await self.test_short_term_memory()
        await self.test_memory_consolidation()
        await self.test_ai_integration()
        await self.test_config_manager()
        await self.test_emote_system()

        # New test categories (Phase 2 expansion)
        await self.test_per_server_isolation()
        await self.test_input_validation()
        await self.test_global_state()
        await self.test_user_management()
        await self.test_archive_system()
        await self.test_image_rate_limiting()
        await self.test_channel_configuration()
        await self.test_formatting_handler()
        await self.test_image_generation()
        await self.test_proactive_engagement()
        await self.test_user_identification()

        # Final cleanup verification (catch-all)
        await self.test_cleanup_verification()

        # Generate summary
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r["passed"])
        failed_tests = total_tests - passed_tests
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        summary = {
            "total": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "pass_rate": pass_rate,
            "results": self.results,
            "guild_name": self.guild_name,
            "guild_id": self.guild_id,
            "timestamp": datetime.now().isoformat()
        }

        print(f"\n{'='*60}")
        print(f"Test Suite Complete")
        print(f"Total: {total_tests} | Passed: {passed_tests} | Failed: {failed_tests}")
        print(f"Pass Rate: {pass_rate:.1f}%")
        print(f"{'='*60}\n")

        # Save results to log file
        self._save_test_log(summary)

        return summary

    def _save_test_log(self, summary: Dict):
        """Save test results to logs directory."""
        try:
            # Get logs directory
            logs_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
            os.makedirs(logs_dir, exist_ok=True)

            # Create log filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_filename = f"test_results_{timestamp}.json"
            log_path = os.path.join(logs_dir, log_filename)

            # Save to JSON
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

            print(f"Test results saved to: {log_path}")
        except Exception as e:
            print(f"Warning: Could not save test log: {e}")

    # ==================== DATABASE TESTS ====================

    async def test_database_connection(self):
        """Test database connection and accessibility."""
        category = "Database Connection"

        # Test 1: Database manager exists
        try:
            exists = self.db_manager is not None
            self._log_test(
                category,
                "Database Manager Initialization",
                exists,
                "Database manager successfully retrieved" if exists else "Failed to get database manager"
            )
        except Exception as e:
            self._log_test(category, "Database Manager Initialization", False, f"Error: {e}")

        # Test 2: Database file exists
        try:
            if self.db_manager:
                db_path = self.db_manager.db_path
                file_exists = os.path.exists(db_path)
                self._log_test(
                    category,
                    "Database File Existence",
                    file_exists,
                    f"Database file at: {db_path}" if file_exists else f"File not found: {db_path}"
                )
        except Exception as e:
            self._log_test(category, "Database File Existence", False, f"Error: {e}")

        # Test 3: Can execute simple query
        try:
            if self.db_manager:
                cursor = self.db_manager.conn.cursor()
                cursor.execute("SELECT 1")
                result = cursor.fetchone()
                success = result == (1,)
                self._log_test(
                    category,
                    "Basic Query Execution",
                    success,
                    "Simple query executed successfully" if success else "Query returned unexpected result"
                )
        except Exception as e:
            self._log_test(category, "Basic Query Execution", False, f"Error: {e}")

    async def test_database_tables(self):
        """Test that all required tables exist."""
        category = "Database Tables"

        required_tables = [
            "bot_identity",
            "relationship_metrics",
            "long_term_memory",
            "global_state",
            "short_term_message_log",
            "users"
        ]

        for table_name in required_tables:
            try:
                cursor = self.db_manager.conn.cursor()
                cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
                result = cursor.fetchone()
                exists = result is not None
                self._log_test(
                    category,
                    f"Table: {table_name}",
                    exists,
                    f"Table '{table_name}' exists" if exists else f"Table '{table_name}' missing"
                )
            except Exception as e:
                self._log_test(category, f"Table: {table_name}", False, f"Error: {e}")

    # ==================== BOT IDENTITY TESTS ====================

    async def test_bot_identity(self):
        """Test bot identity storage and retrieval."""
        category = "Bot Identity"

        # Test 1: Can retrieve bot identity
        try:
            traits = self.db_manager.get_bot_identity("trait")
            lore = self.db_manager.get_bot_identity("lore")
            facts = self.db_manager.get_bot_identity("fact")

            has_content = len(traits) > 0 or len(lore) > 0 or len(facts) > 0
            self._log_test(
                category,
                "Retrieve Bot Identity",
                has_content,
                f"Found {len(traits)} traits, {len(lore)} lore, {len(facts)} facts" if has_content else "No identity data found"
            )
        except Exception as e:
            self._log_test(category, "Retrieve Bot Identity", False, f"Error: {e}")

        # Test 2: Can add and retrieve test trait
        try:
            test_trait = f"TEST_TRAIT_{datetime.now().timestamp()}"
            self.db_manager.add_bot_identity("trait", test_trait)

            traits = self.db_manager.get_bot_identity("trait")  # Returns list of strings
            added = test_trait in traits

            # Clean up test data
            if added:
                cursor = self.db_manager.conn.cursor()
                cursor.execute("DELETE FROM bot_identity WHERE content = ?", (test_trait,))
                self.db_manager.conn.commit()

            self._log_test(
                category,
                "Add/Retrieve Bot Identity",
                added,
                "Successfully added and retrieved test trait" if added else "Failed to add or retrieve test trait"
            )
        except Exception as e:
            self._log_test(category, "Add/Retrieve Bot Identity", False, f"Error: {e}")

    # ==================== RELATIONSHIP METRICS TESTS ====================

    async def test_relationship_metrics(self):
        """Test relationship metrics operations."""
        category = "Relationship Metrics"

        test_user_id = 999999999999999999  # Fake user ID for testing

        # Test 1: Auto-create metrics for new user (via get)
        try:
            # Ensure user exists in users table first
            self.db_manager._ensure_user_exists(test_user_id)

            metrics = self.db_manager.get_relationship_metrics(test_user_id)  # Auto-creates if not exists

            initialized = metrics is not None and all(k in metrics for k in ['anger', 'rapport', 'trust', 'formality'])
            self._log_test(
                category,
                "Auto-Create User Metrics",
                initialized,
                f"Metrics initialized: {metrics}" if initialized else "Failed to initialize metrics"
            )
        except Exception as e:
            self._log_test(category, "Auto-Create User Metrics", False, f"Error: {e}")

        # Test 2: Update metrics
        try:
            self.db_manager.update_relationship_metrics(
                test_user_id,
                rapport=8,
                trust=7,
                anger=2,
                formality=-3
            )

            metrics = self.db_manager.get_relationship_metrics(test_user_id)
            updated = (
                metrics["rapport"] == 8 and
                metrics["trust"] == 7 and
                metrics["anger"] == 2 and
                metrics["formality"] == -3
            )

            self._log_test(
                category,
                "Update Relationship Metrics",
                updated,
                f"Metrics updated correctly: {metrics}" if updated else f"Metrics mismatch: {metrics}"
            )
        except Exception as e:
            self._log_test(category, "Update Relationship Metrics", False, f"Error: {e}")

        # Test 3: Metric locks functionality
        try:
            # Set locks for some metrics
            self.db_manager.update_relationship_metrics(
                test_user_id,
                respect_locks=False,  # Bypass locks for initial setup
                rapport=5,
                rapport_locked=1,
                anger=3,
                anger_locked=1
            )

            # Try to update locked metrics (should be blocked)
            self.db_manager.update_relationship_metrics(
                test_user_id,
                respect_locks=True,
                rapport=9,  # Locked, shouldn't change
                anger=8,    # Locked, shouldn't change
                trust=6     # Not locked, should change
            )

            # Verify locked metrics didn't change, but unlocked did
            metrics = self.db_manager.get_relationship_metrics(test_user_id)
            locks_work = (
                metrics["rapport"] == 5 and  # Should stay 5 (locked)
                metrics["anger"] == 3 and    # Should stay 3 (locked)
                metrics["trust"] == 6        # Should change to 6 (not locked)
            )

            self._log_test(
                category,
                "Metric Lock Functionality",
                locks_work,
                f"Locked metrics protected: rapport={metrics['rapport']} (locked), anger={metrics['anger']} (locked), trust={metrics['trust']} (unlocked)" if locks_work else f"Lock protection failed: {metrics}"
            )
        except Exception as e:
            self._log_test(category, "Metric Lock Functionality", False, f"Error: {e}")

        # Test 4: Get all users with metrics
        try:
            all_users = self.db_manager.get_all_users_with_metrics()
            has_test_user = any(u['user_id'] == test_user_id for u in all_users)

            self._log_test(
                category,
                "Get All Users With Metrics",
                has_test_user,
                f"Found {len(all_users)} users including test user" if has_test_user else f"Test user not found in {len(all_users)} users"
            )
        except Exception as e:
            self._log_test(category, "Get All Users With Metrics", False, f"Error: {e}")

        # Test 5: Metric lock columns exist
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("PRAGMA table_info(relationship_metrics)")
            columns = [row[1] for row in cursor.fetchall()]
            cursor.close()

            lock_columns = ['rapport_locked', 'anger_locked', 'trust_locked', 'formality_locked',
                            'fear_locked', 'respect_locked', 'affection_locked', 'familiarity_locked', 'intimidation_locked']
            all_exist = all(col in columns for col in lock_columns)

            self._log_test(
                category,
                "Metric Lock Columns Exist",
                all_exist,
                "All lock columns present" if all_exist else f"Missing lock columns: {[c for c in lock_columns if c not in columns]}"
            )
        except Exception as e:
            self._log_test(category, "Metric Lock Columns Exist", False, f"Error: {e}")

        # Test 6a: New expanded metrics columns exist (2025-10-16)
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("PRAGMA table_info(relationship_metrics)")
            columns = [row[1] for row in cursor.fetchall()]
            cursor.close()

            new_metrics = ['fear', 'respect', 'affection', 'familiarity', 'intimidation']
            all_exist = all(col in columns for col in new_metrics)

            self._log_test(
                category,
                "Expanded Metric Columns Exist",
                all_exist,
                "All expanded metric columns present" if all_exist else f"Missing columns: {[c for c in new_metrics if c not in columns]}"
            )
        except Exception as e:
            self._log_test(category, "Expanded Metric Columns Exist", False, f"Error: {e}")

        # Test 6b: New metrics can be set and retrieved
        try:
            self.db_manager.update_relationship_metrics(
                test_user_id,
                respect_locks=False,
                fear=7,
                respect=8,
                affection=6,
                familiarity=9,
                intimidation=5
            )

            metrics = self.db_manager.get_relationship_metrics(test_user_id)
            updated = (
                metrics.get("fear") == 7 and
                metrics.get("respect") == 8 and
                metrics.get("affection") == 6 and
                metrics.get("familiarity") == 9 and
                metrics.get("intimidation") == 5
            )

            self._log_test(
                category,
                "Update Expanded Metrics",
                updated,
                f"Expanded metrics updated correctly: fear={metrics.get('fear')}, respect={metrics.get('respect')}, affection={metrics.get('affection')}, familiarity={metrics.get('familiarity')}, intimidation={metrics.get('intimidation')}" if updated else f"Metrics mismatch: {metrics}"
            )
        except Exception as e:
            self._log_test(category, "Update Expanded Metrics", False, f"Error: {e}")

        # Test 7: Clean up test user
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM relationship_metrics WHERE user_id = ?", (test_user_id,))
            cursor.execute("DELETE FROM users WHERE user_id = ?", (test_user_id,))  # Also clean up from users table
            self.db_manager.conn.commit()

            # Check directly with SQL instead of get_relationship_metrics (which auto-creates)
            cursor.execute("SELECT COUNT(*) FROM relationship_metrics WHERE user_id = ?", (test_user_id,))
            count = cursor.fetchone()[0]
            cleaned = count == 0

            self._log_test(
                category,
                "Delete Test Metrics",
                cleaned,
                "Test metrics cleaned up successfully" if cleaned else f"Failed to clean up test metrics ({count} remaining)"
            )
            cursor.close()
        except Exception as e:
            self._log_test(category, "Delete Test Metrics", False, f"Error: {e}")

    # ==================== LONG-TERM MEMORY TESTS ====================

    async def test_long_term_memory(self):
        """Test long-term memory operations."""
        category = "Long-Term Memory"

        test_user_id = 999999999999999999
        test_fact = f"User likes testing and validation TEST_{datetime.now().timestamp()}"

        # Test 1: Add memory
        try:
            # Ensure user exists first (required for foreign key constraints)
            self.db_manager._ensure_user_exists(test_user_id)

            result = self.db_manager.add_long_term_memory(
                user_id=test_user_id,
                fact=test_fact,
                source_user_id=test_user_id,
                source_nickname="TestUser"
            )

            memories = self.db_manager.get_long_term_memory(test_user_id)  # Returns list of tuples: (fact, source_user_id, source_nickname)
            added = result and any(m[0] == test_fact for m in memories)

            self._log_test(
                category,
                "Add Long-Term Memory",
                added,
                f"Memory added successfully" if added else f"Failed to add memory (result={result})"
            )
        except Exception as e:
            self._log_test(category, "Add Long-Term Memory", False, f"Error: {e}")

        # Test 2: Find contradictory memory
        try:
            similar_memories = self.db_manager.find_contradictory_memory(test_user_id, test_fact)
            found = len(similar_memories) > 0

            self._log_test(
                category,
                "Find Contradictory Memory",
                found,
                f"Found {len(similar_memories)} similar memories" if found else "No similar memories found"
            )
        except Exception as e:
            self._log_test(category, "Find Contradictory Memory", False, f"Error: {e}")

        # Test 3: Update memory (use find_contradictory_memory to get ID)
        try:
            similar_memories = self.db_manager.find_contradictory_memory(test_user_id, test_fact)  # Returns [(id, fact), ...]

            if similar_memories:
                test_memory_id = similar_memories[0][0]  # Get first ID
                updated_fact = f"UPDATED_{test_fact}"
                self.db_manager.update_long_term_memory_fact(test_memory_id, updated_fact)

                memories = self.db_manager.get_long_term_memory(test_user_id)
                updated = any(m[0] == updated_fact for m in memories)

                self._log_test(
                    category,
                    "Update Long-Term Memory",
                    updated,
                    "Memory updated successfully" if updated else "Failed to update memory"
                )
            else:
                self._log_test(category, "Update Long-Term Memory", False, "Test memory not found")
        except Exception as e:
            self._log_test(category, "Update Long-Term Memory", False, f"Error: {e}")

        # Test 4: Delete memory
        try:
            similar_memories = self.db_manager.find_contradictory_memory(test_user_id, "testing and validation TEST_")
            test_memories = [m for m in similar_memories if "TEST_" in m[1]]

            for memory_id, memory_fact in test_memories:
                self.db_manager.delete_long_term_memory_fact(memory_id)

            memories = self.db_manager.get_long_term_memory(test_user_id)
            cleaned = not any("TEST_" in m[0] for m in memories)

            # Also clean up the test user
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM users WHERE user_id = ?", (test_user_id,))
            self.db_manager.conn.commit()
            cursor.close()

            self._log_test(
                category,
                "Delete Long-Term Memory",
                cleaned,
                f"Deleted {len(test_memories)} test memories" if cleaned else "Failed to delete test memories"
            )
        except Exception as e:
            self._log_test(category, "Delete Long-Term Memory", False, f"Error: {e}")

    # ==================== SHORT-TERM MEMORY TESTS ====================

    async def test_short_term_memory(self):
        """Test short-term memory operations."""
        category = "Short-Term Memory"

        test_message_id = 999999999999999999
        test_user_id = 999999999999999999

        # Test 1: Log message (direct SQL since log_message requires Discord message object)
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute(
                "INSERT INTO short_term_message_log (message_id, user_id, channel_id, content, timestamp, directed_at_bot) VALUES (?, ?, ?, ?, ?, ?)",
                (test_message_id, test_user_id, 999999999999999999, "This is a test message", datetime.now().isoformat(), 0)
            )
            self.db_manager.conn.commit()

            messages = self.db_manager.get_short_term_memory()
            logged = any(m["message_id"] == test_message_id for m in messages)

            self._log_test(
                category,
                "Log Message",
                logged,
                "Message logged successfully" if logged else "Failed to log message"
            )
        except Exception as e:
            self._log_test(category, "Log Message", False, f"Error: {e}")

        # Test 2: Retrieve short-term memory
        try:
            messages = self.db_manager.get_short_term_memory()
            retrieved = messages is not None and len(messages) >= 0

            self._log_test(
                category,
                "Retrieve Short-Term Memory",
                retrieved,
                f"Retrieved {len(messages)} messages" if retrieved else "Failed to retrieve messages"
            )
        except Exception as e:
            self._log_test(category, "Retrieve Short-Term Memory", False, f"Error: {e}")

        # Test 3: Clean up test message
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM short_term_message_log WHERE message_id = ?", (test_message_id,))
            self.db_manager.conn.commit()

            messages = self.db_manager.get_short_term_memory()
            cleaned = not any(m["message_id"] == test_message_id for m in messages)

            self._log_test(
                category,
                "Delete Test Message",
                cleaned,
                "Test message cleaned up successfully" if cleaned else "Failed to clean up test message"
            )
        except Exception as e:
            self._log_test(category, "Delete Test Message", False, f"Error: {e}")

    # ==================== MEMORY CONSOLIDATION TESTS ====================

    async def test_memory_consolidation(self):
        """Test memory consolidation system."""
        category = "Memory Consolidation"

        # Test 1: Archive directory exists
        try:
            archive_dir = os.path.join(os.path.dirname(self.db_manager.db_path), "archive")
            exists = os.path.exists(archive_dir)

            self._log_test(
                category,
                "Archive Directory Exists",
                exists,
                f"Archive directory at: {archive_dir}" if exists else f"Archive directory missing: {archive_dir}"
            )
        except Exception as e:
            self._log_test(category, "Archive Directory Exists", False, f"Error: {e}")

        # Test 2: Check if consolidation function exists
        try:
            from cogs.memory_tasks import MemoryTasksCog
            cog = self.bot.get_cog("MemoryTasksCog")
            has_method = cog is not None and hasattr(cog, "consolidate_memories")

            self._log_test(
                category,
                "Consolidation Function Exists",
                has_method,
                "consolidate_memories method found" if has_method else "consolidate_memories method missing"
            )
        except Exception as e:
            self._log_test(category, "Consolidation Function Exists", False, f"Error: {e}")

    # ==================== AI INTEGRATION TESTS ====================

    async def test_ai_integration(self):
        """Test AI handler integration."""
        category = "AI Integration"

        # Test 1: AI Handler exists
        try:
            has_handler = hasattr(self.bot, "ai_handler") and self.bot.ai_handler is not None
            self._log_test(
                category,
                "AI Handler Initialization",
                has_handler,
                "AI Handler exists and initialized" if has_handler else "AI Handler missing"
            )
        except Exception as e:
            self._log_test(category, "AI Handler Initialization", False, f"Error: {e}")

        # Test 2: OpenAI API key configured
        try:
            api_key = self.bot.config_manager.get_secret("OPENAI_API_KEY")
            configured = api_key is not None and len(api_key) > 0

            self._log_test(
                category,
                "OpenAI API Key Configured",
                configured,
                "API key configured" if configured else "API key missing"
            )
        except Exception as e:
            self._log_test(category, "OpenAI API Key Configured", False, f"Error: {e}")

        # Test 3: AI model configuration
        try:
            config = self.bot.config_manager.get_config()
            has_models = "ai_models" in config and "primary_model" in config["ai_models"]

            model_name = config["ai_models"]["primary_model"] if has_models else "Not configured"

            self._log_test(
                category,
                "AI Model Configuration",
                has_models,
                f"Primary model: {model_name}" if has_models else "AI models not configured"
            )
        except Exception as e:
            self._log_test(category, "AI Model Configuration", False, f"Error: {e}")

    # ==================== CONFIG MANAGER TESTS ====================

    async def test_config_manager(self):
        """Test configuration manager."""
        category = "Config Manager"

        # Test 1: Config manager exists
        try:
            has_manager = hasattr(self.bot, "config_manager") and self.bot.config_manager is not None
            self._log_test(
                category,
                "Config Manager Initialization",
                has_manager,
                "Config Manager exists" if has_manager else "Config Manager missing"
            )
        except Exception as e:
            self._log_test(category, "Config Manager Initialization", False, f"Error: {e}")

        # Test 2: Can load config
        try:
            config = self.bot.config_manager.get_config()
            loaded = config is not None and isinstance(config, dict)

            self._log_test(
                category,
                "Load Configuration",
                loaded,
                f"Config loaded with {len(config)} keys" if loaded else "Failed to load config"
            )
        except Exception as e:
            self._log_test(category, "Load Configuration", False, f"Error: {e}")

        # Test 3: Required config keys exist
        try:
            config = self.bot.config_manager.get_config()
            required_keys = ["ai_models", "response_limits", "personality_mode"]
            missing_keys = [key for key in required_keys if key not in config]

            has_all = len(missing_keys) == 0

            self._log_test(
                category,
                "Required Config Keys",
                has_all,
                "All required keys present" if has_all else f"Missing keys: {missing_keys}"
            )
        except Exception as e:
            self._log_test(category, "Required Config Keys", False, f"Error: {e}")

    # ==================== EMOTE SYSTEM TESTS ====================

    async def test_emote_system(self):
        """Test emote orchestrator."""
        category = "Emote System"

        # Test 1: Emote handler exists (attribute is emote_handler, not emote_orchestrator)
        try:
            has_handler = hasattr(self.bot, "emote_handler") and self.bot.emote_handler is not None
            self._log_test(
                category,
                "Emote Handler Initialization",
                has_handler,
                "Emote Handler exists" if has_handler else "Emote Handler missing"
            )
        except Exception as e:
            self._log_test(category, "Emote Handler Initialization", False, f"Error: {e}")

        # Test 2: Can load emotes
        try:
            if hasattr(self.bot, "emote_handler"):
                emote_count = len(self.bot.emote_handler.emotes)  # Attribute is 'emotes', not 'emote_map'
                loaded = emote_count >= 0

                self._log_test(
                    category,
                    "Load Emotes",
                    loaded,
                    f"Loaded {emote_count} emotes" if loaded else "Failed to load emotes"
                )
            else:
                self._log_test(category, "Load Emotes", False, "Emote handler not available")
        except Exception as e:
            self._log_test(category, "Load Emotes", False, f"Error: {e}")

    # ==================== PER-SERVER ISOLATION TESTS ====================

    async def test_per_server_isolation(self):
        """Test per-server database isolation architecture."""
        category = "Per-Server Isolation"

        # Test 1: Database file has server-specific name
        try:
            db_path = self.db_manager.db_path
            has_server_name = self.guild_name.replace(" ", "_")[:50] in db_path or "_data.db" in db_path

            self._log_test(
                category,
                "Server-Specific Database File",
                has_server_name,
                f"Database path: {db_path}" if has_server_name else f"Path doesn't reflect server isolation: {db_path}"
            )
        except Exception as e:
            self._log_test(category, "Server-Specific Database File", False, f"Error: {e}")

        # Test 2: Multi-database manager exists
        try:
            has_multi_db = hasattr(self.bot, "multi_db_manager") or hasattr(self.bot, "get_server_db")

            self._log_test(
                category,
                "Multi-Database Manager",
                has_multi_db,
                "Multi-database manager available" if has_multi_db else "Multi-database manager missing"
            )
        except Exception as e:
            self._log_test(category, "Multi-Database Manager", False, f"Error: {e}")

        # Test 3: Database path is in database directory
        try:
            db_path = self.db_manager.db_path
            in_db_dir = "database" in db_path.lower()

            self._log_test(
                category,
                "Database Directory Structure",
                in_db_dir,
                f"Database in correct directory: {db_path}" if in_db_dir else f"Database not in 'database' directory: {db_path}"
            )
        except Exception as e:
            self._log_test(category, "Database Directory Structure", False, f"Error: {e}")

        # Test 4: SQLite auto-vacuum enabled
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("PRAGMA auto_vacuum")
            result = cursor.fetchone()
            auto_vacuum_enabled = result and result[0] in (1, 2)  # 1=FULL, 2=INCREMENTAL
            cursor.close()

            self._log_test(
                category,
                "SQLite Auto-Vacuum Enabled",
                auto_vacuum_enabled,
                f"Auto-vacuum mode: {result[0] if result else 'OFF'}" if auto_vacuum_enabled else "Auto-vacuum not enabled"
            )
        except Exception as e:
            self._log_test(category, "SQLite Auto-Vacuum Enabled", False, f"Error: {e}")

    # ==================== INPUT VALIDATION TESTS ====================

    async def test_input_validation(self):
        """Test input validation and security measures."""
        category = "Input Validation"

        # Test 1: SQL injection prevention in bot identity
        try:
            malicious_input = "'; DROP TABLE bot_identity; --"
            result = self.db_manager.add_bot_identity("trait", malicious_input)

            # Check if table still exists after attempt
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='bot_identity'")
            table_exists = cursor.fetchone() is not None
            cursor.close()

            # Clean up if it was added
            if result:
                cursor = self.db_manager.conn.cursor()
                cursor.execute("DELETE FROM bot_identity WHERE content = ?", (malicious_input,))
                self.db_manager.conn.commit()
                cursor.close()

            self._log_test(
                category,
                "SQL Injection Prevention",
                table_exists,
                "Table protected from SQL injection" if table_exists else "SQL injection vulnerability detected"
            )
        except Exception as e:
            self._log_test(category, "SQL Injection Prevention", False, f"Error: {e}")

        # Test 2: Oversized content rejection
        try:
            oversized_content = "A" * 100000  # 100KB string
            result = self.db_manager.add_bot_identity("trait", oversized_content)

            # Check if it was rejected or truncated
            traits = self.db_manager.get_bot_identity("trait")
            not_added = oversized_content not in traits

            # Clean up if somehow added
            if not not_added:
                cursor = self.db_manager.conn.cursor()
                cursor.execute("DELETE FROM bot_identity WHERE content = ?", (oversized_content,))
                self.db_manager.conn.commit()
                cursor.close()

            self._log_test(
                category,
                "Oversized Content Rejection",
                not_added,
                "Oversized content rejected" if not_added else "Oversized content was accepted"
            )
        except Exception as e:
            self._log_test(category, "Oversized Content Rejection", False, f"Error: {e}")

        # Test 3: Empty/null input handling
        try:
            empty_result = self.db_manager.add_bot_identity("trait", "")
            null_result = self.db_manager.add_bot_identity("trait", None) if None else False

            # Both should be rejected
            rejected = not empty_result and not null_result

            self._log_test(
                category,
                "Empty/Null Input Rejection",
                rejected,
                "Empty and null inputs rejected" if rejected else "Empty or null inputs accepted"
            )
        except Exception as e:
            # Exception is acceptable for null input
            self._log_test(category, "Empty/Null Input Rejection", True, "Null input properly rejected with exception")

        # Test 4: Invalid category rejection
        try:
            invalid_category = "INVALID_CATEGORY_123"
            result = self.db_manager.add_bot_identity(invalid_category, "test content")

            # Should be rejected
            rejected = not result

            self._log_test(
                category,
                "Invalid Category Rejection",
                rejected,
                "Invalid category rejected" if rejected else "Invalid category was accepted"
            )
        except Exception as e:
            self._log_test(category, "Invalid Category Rejection", False, f"Error: {e}")

    # ==================== GLOBAL STATE TESTS ====================

    async def test_global_state(self):
        """Test global state management system."""
        category = "Global State"

        test_key = f"TEST_STATE_{datetime.now().timestamp()}"
        test_value = "TEST_VALUE_123"

        # Test 1: Set global state
        try:
            self.db_manager.set_global_state(test_key, test_value)

            # Verify it was set
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT state_value FROM global_state WHERE state_key = ?", (test_key,))
            result = cursor.fetchone()
            cursor.close()

            success = result is not None and result[0] == test_value

            self._log_test(
                category,
                "Set Global State",
                success,
                f"State set successfully: {test_key}={test_value}" if success else "Failed to set global state"
            )
        except Exception as e:
            self._log_test(category, "Set Global State", False, f"Error: {e}")

        # Test 2: Get global state
        try:
            retrieved_value = self.db_manager.get_global_state(test_key)
            success = retrieved_value == test_value

            self._log_test(
                category,
                "Get Global State",
                success,
                f"Retrieved value: {retrieved_value}" if success else f"Value mismatch: expected {test_value}, got {retrieved_value}"
            )
        except Exception as e:
            self._log_test(category, "Get Global State", False, f"Error: {e}")

        # Test 3: Clean up test state
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM global_state WHERE state_key = ?", (test_key,))
            self.db_manager.conn.commit()

            # Verify deletion
            result = self.db_manager.get_global_state(test_key)
            cleaned = result is None
            cursor.close()

            self._log_test(
                category,
                "Delete Global State",
                cleaned,
                "Test state cleaned successfully" if cleaned else "Failed to clean test state"
            )
        except Exception as e:
            self._log_test(category, "Delete Global State", False, f"Error: {e}")

    # ==================== USER MANAGEMENT TESTS ====================

    async def test_user_management(self):
        """Test user management and tracking system."""
        category = "User Management"

        test_user_id = 888888888888888888  # Different from other test IDs

        # Test 1: Ensure user exists functionality
        try:
            self.db_manager._ensure_user_exists(test_user_id)

            # Check if user was created
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT user_id, first_seen, last_seen FROM users WHERE user_id = ?", (test_user_id,))
            result = cursor.fetchone()
            cursor.close()

            created = result is not None and result[0] == test_user_id

            self._log_test(
                category,
                "Ensure User Exists",
                created,
                f"User record created for {test_user_id}" if created else "Failed to create user record"
            )
        except Exception as e:
            self._log_test(category, "Ensure User Exists", False, f"Error: {e}")

        # Test 2: User timestamps
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT first_seen, last_seen FROM users WHERE user_id = ?", (test_user_id,))
            result = cursor.fetchone()
            cursor.close()

            has_timestamps = result is not None and result[0] and result[1]

            self._log_test(
                category,
                "User Timestamps",
                has_timestamps,
                f"Timestamps: first_seen={result[0][:19] if result else 'N/A'}, last_seen={result[1][:19] if result else 'N/A'}" if has_timestamps else "Missing timestamps"
            )
        except Exception as e:
            self._log_test(category, "User Timestamps", False, f"Error: {e}")

        # Test 3: Clean up test user
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM users WHERE user_id = ?", (test_user_id,))
            self.db_manager.conn.commit()

            # Verify deletion
            cursor.execute("SELECT COUNT(*) FROM users WHERE user_id = ?", (test_user_id,))
            count = cursor.fetchone()[0]
            cursor.close()

            cleaned = count == 0

            self._log_test(
                category,
                "Delete Test User",
                cleaned,
                "Test user cleaned successfully" if cleaned else "Failed to clean test user"
            )
        except Exception as e:
            self._log_test(category, "Delete Test User", False, f"Error: {e}")

    # ==================== ARCHIVE SYSTEM TESTS ====================

    async def test_archive_system(self):
        """Test message archival and cleanup system."""
        category = "Archive System"

        # Test 1: Archive directory accessibility
        try:
            archive_dir = os.path.join(os.path.dirname(self.db_manager.db_path), "archive")

            # Check if accessible (may or may not exist yet)
            parent_exists = os.path.exists(os.path.dirname(archive_dir))
            accessible = parent_exists  # Parent dir must exist for archiving to work

            self._log_test(
                category,
                "Archive Directory Accessible",
                accessible,
                f"Archive parent directory exists: {os.path.dirname(archive_dir)}" if accessible else "Archive parent directory not accessible"
            )
        except Exception as e:
            self._log_test(category, "Archive Directory Accessible", False, f"Error: {e}")

        # Test 2: Archive function exists
        try:
            has_archive_method = hasattr(self.db_manager, "archive_and_clear_short_term_memory")

            self._log_test(
                category,
                "Archive Function Exists",
                has_archive_method,
                "archive_and_clear_short_term_memory method found" if has_archive_method else "Archive method missing"
            )
        except Exception as e:
            self._log_test(category, "Archive Function Exists", False, f"Error: {e}")

        # Test 3: Message count method
        try:
            count = self.db_manager.get_short_term_message_count()
            has_method = True

            self._log_test(
                category,
                "Message Count Method",
                has_method,
                f"Current message count: {count}" if has_method else "Message count method missing"
            )
        except Exception as e:
            self._log_test(category, "Message Count Method", False, f"Error: {e}")

        # Test 4: Archive JSON format validation
        try:
            archive_dir = os.path.join(os.path.dirname(self.db_manager.db_path), "archive")

            # Check if any archive files exist and validate format
            if os.path.exists(archive_dir):
                archive_files = [f for f in os.listdir(archive_dir) if f.startswith("short_term_archive_") and f.endswith(".json")]

                if archive_files:
                    # Read most recent archive
                    latest_archive = sorted(archive_files)[-1]
                    archive_path = os.path.join(archive_dir, latest_archive)

                    with open(archive_path, 'r', encoding='utf-8') as f:
                        archive_data = json.load(f)

                    # Validate required keys
                    valid_format = all(key in archive_data for key in ["archived_at", "message_count", "messages"])

                    self._log_test(
                        category,
                        "Archive JSON Format",
                        valid_format,
                        f"Valid archive format in {latest_archive}" if valid_format else "Archive format invalid"
                    )
                else:
                    self._log_test(category, "Archive JSON Format", True, "No archive files yet (normal for new servers)")
            else:
                self._log_test(category, "Archive JSON Format", True, "Archive directory not created yet (normal)")
        except Exception as e:
            self._log_test(category, "Archive JSON Format", False, f"Error: {e}")

    # ==================== IMAGE RATE LIMITING TESTS ====================

    async def test_image_rate_limiting(self):
        """Test image rate limiting system."""
        category = "Image Rate Limiting"

        test_user_id = 777777777777777777  # Different test ID

        # Test 1: Image count tracking table exists
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='user_image_stats'")
            exists = cursor.fetchone() is not None
            cursor.close()

            self._log_test(
                category,
                "Image Stats Table Exists",
                exists,
                "user_image_stats table found" if exists else "user_image_stats table missing"
            )
        except Exception as e:
            self._log_test(category, "Image Stats Table Exists", False, f"Error: {e}")

        # Test 2: Increment image count method
        try:
            has_method = hasattr(self.db_manager, "increment_user_image_count")
            result = None

            if has_method:
                # Ensure user exists first (required for foreign key constraint)
                self.db_manager._ensure_user_exists(test_user_id)

                # Clean up any existing record first
                cursor = self.db_manager.conn.cursor()
                cursor.execute("DELETE FROM user_image_stats WHERE user_id = ?", (test_user_id,))
                self.db_manager.conn.commit()
                cursor.close()

                # Test the method
                self.db_manager.increment_user_image_count(test_user_id)

                # Give a moment for commit to complete
                import time
                time.sleep(0.1)

                # Verify it was incremented
                cursor = self.db_manager.conn.cursor()
                cursor.execute("SELECT hourly_count, daily_count FROM user_image_stats WHERE user_id = ?", (test_user_id,))
                result = cursor.fetchone()

                # Debug: Check if table has any data at all
                cursor.execute("SELECT COUNT(*) FROM user_image_stats")
                total_count = cursor.fetchone()[0]
                cursor.close()

                success = result is not None and result[0] >= 1 and result[1] >= 1

                detail_msg = f"Image count incremented: hourly={result[0]}, daily={result[1]}" if (success and result) else f"Failed: result={result}, total_records={total_count}"
            else:
                success = False
                detail_msg = "Increment method missing"

            self._log_test(
                category,
                "Increment Image Count",
                success,
                detail_msg
            )
        except Exception as e:
            self._log_test(category, "Increment Image Count", False, f"Error: {e}")

        # Test 3: Get hourly count method
        try:
            has_method = hasattr(self.db_manager, "get_user_image_count_last_hour")

            if has_method:
                count = self.db_manager.get_user_image_count_last_hour(test_user_id)
                success = count >= 0  # Should return at least 0
            else:
                success = False

            self._log_test(
                category,
                "Get Hourly Image Count",
                success,
                f"Hourly count retrieved: {count if has_method else 'N/A'}" if success else "Get hourly count method missing"
            )
        except Exception as e:
            self._log_test(category, "Get Hourly Image Count", False, f"Error: {e}")

        # Test 4: Clean up test image stats
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("DELETE FROM user_image_stats WHERE user_id = ?", (test_user_id,))
            cursor.execute("DELETE FROM users WHERE user_id = ?", (test_user_id,))  # Also clean up user
            self.db_manager.conn.commit()

            # Verify deletion
            cursor.execute("SELECT COUNT(*) FROM user_image_stats WHERE user_id = ?", (test_user_id,))
            count = cursor.fetchone()[0]
            cursor.close()

            cleaned = count == 0

            self._log_test(
                category,
                "Delete Test Image Stats",
                cleaned,
                "Test image stats cleaned" if cleaned else "Failed to clean test image stats"
            )
        except Exception as e:
            self._log_test(category, "Delete Test Image Stats", False, f"Error: {e}")

    # ==================== CHANNEL CONFIGURATION TESTS ====================

    async def test_channel_configuration(self):
        """Test channel-specific configuration system."""
        category = "Channel Configuration"

        # Test 1: Config manager has channel settings
        try:
            config = self.bot.config_manager.get_config()
            has_channel_settings = "channel_settings" in config

            self._log_test(
                category,
                "Channel Settings in Config",
                has_channel_settings,
                f"Channel settings section exists with {len(config.get('channel_settings', {}))} channels configured" if has_channel_settings else "Channel settings section missing"
            )
        except Exception as e:
            self._log_test(category, "Channel Settings in Config", False, f"Error: {e}")

        # Test 2: Personality mode configuration exists
        try:
            config = self.bot.config_manager.get_config()
            has_personality_mode = "personality_mode" in config

            if has_personality_mode:
                pm_config = config["personality_mode"]
                has_required_keys = all(key in pm_config for key in ["immersive_character", "allow_technical_language"])
            else:
                has_required_keys = False

            self._log_test(
                category,
                "Personality Mode Configuration",
                has_required_keys,
                f"Personality mode config: immersive={pm_config.get('immersive_character')}, technical={pm_config.get('allow_technical_language')}" if has_required_keys else "Personality mode config incomplete"
            )
        except Exception as e:
            self._log_test(category, "Personality Mode Configuration", False, f"Error: {e}")

        # Test 3: Server info directory exists
        try:
            server_info_dir = os.path.join(os.path.dirname(os.path.dirname(self.db_manager.db_path)), "Server_Info")
            exists = os.path.exists(server_info_dir) or os.path.exists("Server_Info")

            self._log_test(
                category,
                "Server Info Directory",
                True,  # Always pass - directory is optional
                f"Server Info directory {'exists' if exists else 'not created (optional)'}: {server_info_dir if exists else 'Server_Info/'}"
            )
        except Exception as e:
            self._log_test(category, "Server Info Directory", False, f"Error: {e}")

    # ==================== FORMATTING HANDLER TESTS ====================

    async def test_formatting_handler(self):
        """Test roleplay action formatting system."""
        category = "Formatting Handler"

        # Import the formatting handler
        try:
            from modules.formatting_handler import FormattingHandler
            formatter = FormattingHandler()
            import_success = True
        except Exception as e:
            self._log_test(category, "Import FormattingHandler", False, f"Error importing: {e}")
            return

        self._log_test(category, "Import FormattingHandler", True, "Module loaded successfully")

        # Test 1: Format simple action
        try:
            test_input = "walks over to the counter"
            result = formatter.format_actions(test_input, enable_formatting=True)
            expected = "*walks over to the counter*"
            formatted_correctly = result == expected

            self._log_test(
                category,
                "Format Simple Action",
                formatted_correctly,
                f"Input: '{test_input}' -> Output: '{result}'" if formatted_correctly else f"Expected '{expected}', got '{result}'"
            )
        except Exception as e:
            self._log_test(category, "Format Simple Action", False, f"Error: {e}")

        # Test 2: Preserve non-action text
        try:
            test_input = "Hey there! How are you doing?"
            result = formatter.format_actions(test_input, enable_formatting=True)
            preserved = result == test_input

            self._log_test(
                category,
                "Preserve Non-Action Text",
                preserved,
                "Non-action text unchanged" if preserved else f"Text incorrectly modified: '{result}'"
            )
        except Exception as e:
            self._log_test(category, "Preserve Non-Action Text", False, f"Error: {e}")

        # Test 3: Respect formatting disabled flag
        try:
            test_input = "walks over to the counter"
            result = formatter.format_actions(test_input, enable_formatting=False)
            disabled_correctly = result == test_input

            self._log_test(
                category,
                "Respect Disabled Flag",
                disabled_correctly,
                "Formatting correctly disabled" if disabled_correctly else f"Text formatted when disabled: '{result}'"
            )
        except Exception as e:
            self._log_test(category, "Respect Disabled Flag", False, f"Error: {e}")

        # Test 4: Preserve existing formatting
        try:
            test_input = "*waves hello*"
            result = formatter.format_actions(test_input, enable_formatting=True)
            preserved = result == test_input

            self._log_test(
                category,
                "Preserve Existing Formatting",
                preserved,
                "Existing formatting preserved" if preserved else f"Re-formatted existing: '{result}'"
            )
        except Exception as e:
            self._log_test(category, "Preserve Existing Formatting", False, f"Error: {e}")

        # Test 5: Configuration setting exists
        try:
            config = self.bot.config_manager.get_config()
            personality_mode = config.get("personality_mode", {})
            has_setting = "enable_roleplay_formatting" in personality_mode

            self._log_test(
                category,
                "Roleplay Formatting Config",
                has_setting,
                f"Config value: {personality_mode.get('enable_roleplay_formatting')}" if has_setting else "Setting not found in personality_mode"
            )
        except Exception as e:
            self._log_test(category, "Roleplay Formatting Config", False, f"Error: {e}")

        # Test 6: AI Handler integration
        try:
            ai_handler = self.bot.ai_handler
            has_formatter = hasattr(ai_handler, 'formatter')
            has_method = hasattr(ai_handler, '_apply_roleplay_formatting')

            integration_success = has_formatter and has_method

            self._log_test(
                category,
                "AI Handler Integration",
                integration_success,
                "FormattingHandler integrated into AI Handler" if integration_success else f"Missing: formatter={has_formatter}, method={has_method}"
            )
        except Exception as e:
            self._log_test(category, "AI Handler Integration", False, f"Error: {e}")

    # ==================== IMAGE GENERATION TESTS ====================

    async def test_image_generation(self):
        """Test AI image generation system."""
        category = "Image Generation"

        # Test 1: ImageGenerator module exists
        try:
            from modules.image_generator import ImageGenerator
            module_exists = True
            self._log_test(
                category,
                "ImageGenerator Module Import",
                True,
                "ImageGenerator module imported successfully"
            )
        except Exception as e:
            self._log_test(category, "ImageGenerator Module Import", False, f"Error importing: {e}")
            module_exists = False

        # Test 2: Together API key configured
        try:
            together_key = self.bot.config_manager.get_secret("TOGETHER_API_KEY")
            configured = together_key is not None and len(together_key) > 0

            self._log_test(
                category,
                "Together API Key Configured",
                True,  # Always pass - key is optional
                "API key configured" if configured else "API key not set (optional - feature will be disabled)"
            )
        except Exception as e:
            self._log_test(category, "Together API Key Configured", False, f"Error: {e}")

        # Test 3: Image generation config exists
        try:
            config = self.bot.config_manager.get_config()
            has_img_gen_config = "image_generation" in config

            if has_img_gen_config:
                img_config = config["image_generation"]
                has_required_keys = all(key in img_config for key in ["enabled", "max_per_user_per_period", "reset_period_hours", "model"])
            else:
                has_required_keys = False

            self._log_test(
                category,
                "Image Generation Config",
                has_required_keys,
                f"Config found: enabled={img_config.get('enabled')}, limit={img_config.get('max_per_user_per_period')} per {img_config.get('reset_period_hours')}h, model={img_config.get('model')}" if has_required_keys else "Image generation config incomplete"
            )
        except Exception as e:
            self._log_test(category, "Image Generation Config", False, f"Error: {e}")

        # Test 4: AI Handler has ImageGenerator instance
        try:
            if module_exists:
                has_generator = hasattr(self.bot.ai_handler, "image_generator")

                if has_generator:
                    generator = self.bot.ai_handler.image_generator
                    is_available = generator.is_available()

                    self._log_test(
                        category,
                        "ImageGenerator Integration",
                        has_generator,
                        f"ImageGenerator integrated, available={is_available}" if has_generator else "ImageGenerator not found in AI Handler"
                    )
                else:
                    self._log_test(category, "ImageGenerator Integration", False, "image_generator attribute missing from AI Handler")
            else:
                self._log_test(category, "ImageGenerator Integration", False, "Skipped - module import failed")
        except Exception as e:
            self._log_test(category, "ImageGenerator Integration", False, f"Error: {e}")

        # Test 5: Intent classification includes image_generation
        try:
            # Check if AI handler recognizes image_generation intent
            config = self.bot.config_manager.get_config()

            # We can't easily test intent classification without making an API call,
            # but we can verify the intent is in the validation list
            # This is a basic structural check
            has_ai_handler = hasattr(self.bot, "ai_handler")

            self._log_test(
                category,
                "Image Generation Intent",
                has_ai_handler,
                "AI Handler available for intent classification" if has_ai_handler else "AI Handler not available"
            )
        except Exception as e:
            self._log_test(category, "Image Generation Intent", False, f"Error: {e}")

        # Test 6: ImageGenerator methods exist
        try:
            if module_exists:
                from modules.image_generator import ImageGenerator
                generator = ImageGenerator(self.bot.config_manager)

                has_generate = hasattr(generator, "generate_image")
                has_is_available = hasattr(generator, "is_available")
                has_build_prompt = hasattr(generator, "_build_prompt")

                all_methods = has_generate and has_is_available and has_build_prompt

                self._log_test(
                    category,
                    "ImageGenerator Methods",
                    all_methods,
                    "All required methods found: generate_image, is_available, _build_prompt" if all_methods else f"Missing methods: generate={has_generate}, available={has_is_available}, build_prompt={has_build_prompt}"
                )
            else:
                self._log_test(category, "ImageGenerator Methods", False, "Skipped - module import failed")
        except Exception as e:
            self._log_test(category, "ImageGenerator Methods", False, f"Error: {e}")

    # ==================== STATUS UPDATE TESTS ====================

    async def test_status_updates(self):
        """Test AI-generated status update system."""
        category = "Status Updates"

        # Test 1: StatusUpdater module exists
        try:
            from modules.status_updater import StatusUpdater
            module_exists = True
            self._log_test(
                category,
                "StatusUpdater Module Import",
                True,
                "StatusUpdater module imported successfully"
            )
        except Exception as e:
            self._log_test(category, "StatusUpdater Module Import", False, f"Error importing: {e}")
            module_exists = False

        # Test 2: Status updates config exists
        try:
            config = self.bot.config_manager.get_config()
            has_status_config = "status_updates" in config

            if has_status_config:
                status_config = config["status_updates"]
                has_required_keys = all(key in status_config for key in ["enabled", "update_time", "source_server_name"])
            else:
                has_required_keys = False

            self._log_test(
                category,
                "Status Updates Config",
                has_required_keys,
                f"Config found: enabled={status_config.get('enabled')}, time={status_config.get('update_time')}, source={status_config.get('source_server_name')}" if has_required_keys else "Status updates config incomplete"
            )
        except Exception as e:
            self._log_test(category, "Status Updates Config", False, f"Error: {e}")

        # Test 3: Status history file is in gitignore
        try:
            import os
            gitignore_path = os.path.join(os.getcwd(), ".gitignore")

            if os.path.exists(gitignore_path):
                with open(gitignore_path, "r") as f:
                    gitignore_content = f.read()
                    in_gitignore = "status_history.json" in gitignore_content
            else:
                in_gitignore = False

            self._log_test(
                category,
                "Status History in Gitignore",
                in_gitignore,
                "status_history.json is in .gitignore" if in_gitignore else "status_history.json not in .gitignore (should be added)"
            )
        except Exception as e:
            self._log_test(category, "Status History in Gitignore", False, f"Error: {e}")

        # Test 4: Duplicate prevention methods exist
        try:
            if module_exists:
                from modules.status_updater import StatusUpdater
                updater = StatusUpdater(self.bot)

                has_load_history = hasattr(updater, "_load_status_history")
                has_save_history = hasattr(updater, "_save_status_history")
                has_is_duplicate = hasattr(updater, "_is_duplicate_status")

                all_methods = has_load_history and has_save_history and has_is_duplicate

                self._log_test(
                    category,
                    "Duplicate Prevention Methods",
                    all_methods,
                    "All methods found: _load_status_history, _save_status_history, _is_duplicate_status" if all_methods else f"Missing methods: load={has_load_history}, save={has_save_history}, is_duplicate={has_is_duplicate}"
                )
            else:
                self._log_test(category, "Duplicate Prevention Methods", False, "Skipped - module import failed")
        except Exception as e:
            self._log_test(category, "Duplicate Prevention Methods", False, f"Error: {e}")

        # Test 5: Server name autocomplete exists in admin commands
        try:
            from cogs.admin import AdminCog
            has_autocomplete = hasattr(AdminCog, "server_name_autocomplete")

            self._log_test(
                category,
                "Server Name Autocomplete",
                has_autocomplete,
                "server_name_autocomplete method exists in AdminCog" if has_autocomplete else "Autocomplete method missing"
            )
        except Exception as e:
            self._log_test(category, "Server Name Autocomplete", False, f"Error: {e}")

        # Test 6: CustomActivity uses correct constructor (not 'name' parameter)
        try:
            import inspect
            from modules.status_updater import StatusUpdater

            # Read the source code to check for correct CustomActivity usage
            source = inspect.getsource(StatusUpdater.generate_and_update_status)

            # Check that it uses CustomActivity(new_status) not CustomActivity(name=new_status)
            has_old_bug = "CustomActivity(name=" in source
            has_correct_usage = "CustomActivity(new_status)" in source or "CustomActivity(" in source and "name=" not in source

            is_fixed = not has_old_bug and "CustomActivity" in source

            self._log_test(
                category,
                "CustomActivity Constructor Fix",
                is_fixed,
                "CustomActivity uses correct constructor (no 'name' parameter)" if is_fixed else "CustomActivity may be using incorrect constructor with 'name=' parameter"
            )
        except Exception as e:
            self._log_test(category, "CustomActivity Constructor Fix", False, f"Error: {e}")

    # ==================== USER ID RESOLUTION TESTS ====================

    async def test_user_id_resolution(self):
        """Test user ID resolution for admin commands."""
        category = "User ID Resolution"

        # Test 1: _resolve_user helper exists
        try:
            from cogs.admin import AdminCog
            has_resolve_user = hasattr(AdminCog, "_resolve_user")

            self._log_test(
                category,
                "_resolve_user Helper Method",
                has_resolve_user,
                "_resolve_user method exists in AdminCog" if has_resolve_user else "Helper method missing"
            )
        except Exception as e:
            self._log_test(category, "_resolve_user Helper Method", False, f"Error: {e}")

        # Test 2: Test user ID parsing logic
        try:
            # Test parsing various formats
            test_cases = [
                ("123456789", "123456789"),  # Raw ID
                ("<@123456789>", "123456789"),  # Mention format
                ("<@!123456789>", "123456789"),  # Nickname mention format
            ]

            all_passed = True
            for input_str, expected in test_cases:
                # Simulate the parsing logic
                parsed = input_str.strip().replace('<@', '').replace('!', '').replace('>', '')
                if parsed != expected:
                    all_passed = False
                    break

            self._log_test(
                category,
                "User ID Parsing Logic",
                all_passed,
                "All user ID formats parse correctly" if all_passed else "Some formats failed to parse"
            )
        except Exception as e:
            self._log_test(category, "User ID Parsing Logic", False, f"Error: {e}")

        # Test 3: User commands accept string parameter
        try:
            from cogs.admin import AdminCog
            import inspect

            # Check if user_set_metrics accepts string for user parameter
            sig = inspect.signature(AdminCog.user_set_metrics)
            user_param = sig.parameters.get('user')

            # The parameter should be annotated as str
            accepts_string = user_param and user_param.annotation == str

            self._log_test(
                category,
                "User Commands Accept String",
                accepts_string,
                "user_set_metrics accepts str parameter" if accepts_string else "user parameter type incorrect"
            )
        except Exception as e:
            self._log_test(category, "User Commands Accept String", False, f"Error: {e}")

    # ==================== IMAGE NAME STRIPPING TESTS ====================

    async def test_bot_name_stripping(self):
        """Test bot name stripping from image generation prompts."""
        category = "Bot Name Stripping"

        # Test 1: _strip_bot_name_from_prompt method exists
        try:
            has_method = hasattr(self.bot.ai_handler, "_strip_bot_name_from_prompt")

            self._log_test(
                category,
                "_strip_bot_name_from_prompt Method",
                has_method,
                "Method exists in AI Handler" if has_method else "Method missing from AI Handler"
            )
        except Exception as e:
            self._log_test(category, "_strip_bot_name_from_prompt Method", False, f"Error: {e}")

        # Test 2: Test bot name removal logic
        try:
            # Create a mock guild object
            class MockGuild:
                class MockMember:
                    display_name = "TestBot"
                me = MockMember()
                id = 123456789

            mock_guild = MockGuild()

            # Test the stripping logic
            test_prompt = "TestBot, draw me a cat"
            cleaned = self.bot.ai_handler._strip_bot_name_from_prompt(test_prompt, mock_guild)

            # Should remove "TestBot, " from the beginning
            bot_name_removed = "TestBot" not in cleaned

            self._log_test(
                category,
                "Bot Name Removal",
                bot_name_removed,
                f"Bot name stripped successfully: '{test_prompt}' → '{cleaned}'" if bot_name_removed else f"Bot name still present in: '{cleaned}'"
            )
        except Exception as e:
            self._log_test(category, "Bot Name Removal", False, f"Error: {e}")

        # Test 3: Alternative nicknames also stripped
        try:
            config = self.bot.config_manager.get_config()
            has_alt_nicknames_config = "alternative_nicknames" in config or "server_alternative_nicknames" in config

            self._log_test(
                category,
                "Alternative Nicknames Config",
                True,  # Always pass - these are optional
                f"Alternative nicknames config exists: global={('alternative_nicknames' in config)}, server={('server_alternative_nicknames' in config)}"
            )
        except Exception as e:
            self._log_test(category, "Alternative Nicknames Config", False, f"Error: {e}")

    # ==================== PROACTIVE ENGAGEMENT TESTS ====================

    async def test_proactive_engagement(self):
        """Test proactive engagement system to ensure it uses neutral context."""
        category = "Proactive Engagement"

        try:
            # Test 1: Verify generate_proactive_response method exists
            has_method = hasattr(self.bot.ai_handler, 'generate_proactive_response')
            self._log_test(
                category,
                "Proactive Response Method Exists",
                has_method,
                "generate_proactive_response() method found" if has_method else "Method not found"
            )

            if not has_method:
                return

            # Test 2: Verify method signature accepts channel, messages, db_manager
            import inspect
            sig = inspect.signature(self.bot.ai_handler.generate_proactive_response)
            params = list(sig.parameters.keys())
            expected_params = ['channel', 'recent_messages', 'db_manager']
            correct_signature = all(p in params for p in expected_params)

            self._log_test(
                category,
                "Correct Method Signature",
                correct_signature,
                f"Parameters: {params}" if correct_signature else f"Expected {expected_params}, got {params}"
            )

            # Test 3: Verify proactive_engagement module uses new method
            from modules import proactive_engagement
            import inspect as insp
            source = insp.getsource(proactive_engagement.ProactiveEngagement.generate_proactive_response)
            uses_new_method = 'generate_proactive_response' in source and 'ai_handler.generate_proactive_response' in source

            self._log_test(
                category,
                "Proactive Module Uses AI Handler Method",
                uses_new_method,
                "Correctly calls ai_handler.generate_proactive_response()" if uses_new_method else "Still using old generate_response()"
            )

        except Exception as e:
            self._log_test(category, "Proactive Engagement Tests", False, f"Error: {e}")

    # ==================== USER IDENTIFICATION TESTS ====================

    async def test_user_identification(self):
        """Test that all AI response prompts include explicit user identification."""
        category = "User Identification"

        try:
            # Read the ai_handler source code to verify user identification prompts
            with open('modules/ai_handler.py', 'r', encoding='utf-8') as f:
                ai_handler_source = f.read()

            # Test 1: Image generation has user identification
            has_image_user_id = '**CRITICAL - CURRENT USER IDENTIFICATION**' in ai_handler_source and 'drawing_prompt' in ai_handler_source
            self._log_test(
                category,
                "Image Generation User ID",
                has_image_user_id,
                "Image generation prompt includes user identification" if has_image_user_id else "Missing user ID in image generation"
            )

            # Test 2: Casual chat normal metrics has user identification
            has_casual_user_id = '🎯 **CURRENT USER IDENTIFICATION** 🎯' in ai_handler_source
            casual_count = ai_handler_source.count('🎯 **CURRENT USER IDENTIFICATION** 🎯')
            # Should appear in: factual_question, casual_chat_normal, image_success, image_failure, proactive has different marker
            expected_count = 4  # factual_question, casual_chat, image_success, image_failure

            self._log_test(
                category,
                "User ID in Multiple Prompts",
                casual_count >= expected_count,
                f"Found {casual_count} user identification sections (expected at least {expected_count})"
            )

            # Test 3: Proactive engagement has PROACTIVE ENGAGEMENT MODE marker
            has_proactive_marker = '🎯 **PROACTIVE ENGAGEMENT MODE** 🎯' in ai_handler_source
            self._log_test(
                category,
                "Proactive Engagement Neutral Context",
                has_proactive_marker,
                "Proactive engagement has dedicated neutral context prompt" if has_proactive_marker else "Missing proactive engagement marker"
            )

            # Test 4: No fish references in code (generic examples only)
            fish_references = ['fishstrong', 'fishwhat', 'fishreadingemote', 'cookmeafish', 'Dr. Fish', 'dr fish']
            found_fish_refs = []
            for ref in fish_references:
                if ref.lower() in ai_handler_source.lower():
                    found_fish_refs.append(ref)

            no_fish_refs = len(found_fish_refs) == 0
            self._log_test(
                category,
                "No Fish References (Generic Examples)",
                no_fish_refs,
                "All examples use generic placeholders" if no_fish_refs else f"Found fish references: {found_fish_refs}"
            )

            # Test 5: Bot name confusion warnings present
            has_name_warnings = '**NEVER mention your own name or make puns about it.**' in ai_handler_source
            warning_count = ai_handler_source.count('**NEVER mention your own name')

            self._log_test(
                category,
                "Bot Name Confusion Warnings",
                has_name_warnings and warning_count >= 3,
                f"Found {warning_count} bot name warnings in prompts" if has_name_warnings else "Missing bot name warnings"
            )

        except Exception as e:
            self._log_test(category, "User Identification Tests", False, f"Error: {e}")

    # ==================== CLEANUP VERIFICATION TESTS ====================

    async def test_cleanup_verification(self):
        """
        Final verification that all test data was properly cleaned up.
        This is a catch-all to ensure no test data remains in the database.
        """
        category = "Cleanup Verification"

        # All test user IDs used in tests
        test_user_ids = [999999999999999999, 888888888888888888, 777777777777777777]
        test_message_id = 999999999999999999

        # Test 1: No test bot identity entries remain
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute(
                "SELECT COUNT(*) FROM bot_identity WHERE content LIKE ?",
                ("%TEST_TRAIT_%",)
            )
            count = cursor.fetchone()[0]
            cleaned = count == 0

            self._log_test(
                category,
                "Bot Identity Cleanup",
                cleaned,
                "No test identity entries found" if cleaned else f"Found {count} test identity entries remaining"
            )

            # Force cleanup if any remain
            if count > 0:
                cursor.execute("DELETE FROM bot_identity WHERE content LIKE ?", ("%TEST_TRAIT_%",))
                self.db_manager.conn.commit()
                print(f"WARNING: Cleaned up {count} remaining test identity entries")

        except Exception as e:
            self._log_test(category, "Bot Identity Cleanup", False, f"Error: {e}")

        # Test 2: No test relationship metrics remain
        try:
            cursor = self.db_manager.conn.cursor()
            total_count = 0
            for uid in test_user_ids:
                cursor.execute("SELECT COUNT(*) FROM relationship_metrics WHERE user_id = ?", (uid,))
                total_count += cursor.fetchone()[0]

            cleaned = total_count == 0

            self._log_test(
                category,
                "Relationship Metrics Cleanup",
                cleaned,
                "No test metrics found" if cleaned else f"Found {total_count} test metric entries remaining"
            )

            # Force cleanup if any remain
            if total_count > 0:
                for uid in test_user_ids:
                    cursor.execute("DELETE FROM relationship_metrics WHERE user_id = ?", (uid,))
                self.db_manager.conn.commit()
                print(f"WARNING: Cleaned up {total_count} remaining test metric entries")
            cursor.close()

        except Exception as e:
            self._log_test(category, "Relationship Metrics Cleanup", False, f"Error: {e}")

        # Test 3: No test long-term memory entries remain
        try:
            cursor = self.db_manager.conn.cursor()
            total_count = 0

            # Check all test user IDs
            for uid in test_user_ids:
                cursor.execute(
                    "SELECT COUNT(*) FROM long_term_memory WHERE user_id = ?",
                    (uid,)
                )
                total_count += cursor.fetchone()[0]

            # Also check for TEST_ pattern in facts
            cursor.execute(
                "SELECT COUNT(*) FROM long_term_memory WHERE fact LIKE ?",
                ("%TEST_%",)
            )
            total_count += cursor.fetchone()[0]

            cleaned = total_count == 0

            self._log_test(
                category,
                "Long-Term Memory Cleanup",
                cleaned,
                "No test memory entries found" if cleaned else f"Found {total_count} test memory entries remaining"
            )

            # Force cleanup if any remain
            if total_count > 0:
                for uid in test_user_ids:
                    cursor.execute(
                        "DELETE FROM long_term_memory WHERE user_id = ?",
                        (uid,)
                    )
                cursor.execute(
                    "DELETE FROM long_term_memory WHERE fact LIKE ?",
                    ("%TEST_%",)
                )
                self.db_manager.conn.commit()
                print(f"WARNING: Cleaned up {total_count} remaining test memory entries")
            cursor.close()

        except Exception as e:
            self._log_test(category, "Long-Term Memory Cleanup", False, f"Error: {e}")

        # Test 4: No test short-term message log entries remain
        try:
            cursor = self.db_manager.conn.cursor()
            total_count = 0

            # Check test message ID
            cursor.execute(
                "SELECT COUNT(*) FROM short_term_message_log WHERE message_id = ?",
                (test_message_id,)
            )
            total_count += cursor.fetchone()[0]

            # Check all test user IDs
            for uid in test_user_ids:
                cursor.execute(
                    "SELECT COUNT(*) FROM short_term_message_log WHERE user_id = ?",
                    (uid,)
                )
                total_count += cursor.fetchone()[0]

            cleaned = total_count == 0

            self._log_test(
                category,
                "Short-Term Memory Cleanup",
                cleaned,
                "No test messages found" if cleaned else f"Found {total_count} test message entries remaining"
            )

            # Force cleanup if any remain
            if total_count > 0:
                cursor.execute(
                    "DELETE FROM short_term_message_log WHERE message_id = ?",
                    (test_message_id,)
                )
                for uid in test_user_ids:
                    cursor.execute(
                        "DELETE FROM short_term_message_log WHERE user_id = ?",
                        (uid,)
                    )
                self.db_manager.conn.commit()
                print(f"WARNING: Cleaned up {total_count} remaining test message entries")
            cursor.close()

        except Exception as e:
            self._log_test(category, "Short-Term Memory Cleanup", False, f"Error: {e}")

        # Test 5: Final database integrity check
        try:
            cursor = self.db_manager.conn.cursor()

            # Check for any entries with test patterns
            test_patterns = [
                ("bot_identity", "content LIKE '%TEST_%'"),
                ("long_term_memory", "fact LIKE '%TEST_%'"),
                ("short_term_message_log", "content LIKE '%test message%'"),
                ("global_state", "state_key LIKE '%TEST_STATE_%'"),
            ]

            # Add checks for all test user IDs
            for uid in test_user_ids:
                test_patterns.extend([
                    ("relationship_metrics", f"user_id = {uid}"),
                    ("users", f"user_id = {uid}"),
                    ("user_image_stats", f"user_id = {uid}")
                ])

            total_remaining = 0
            details = []

            for table, condition in test_patterns:
                cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE {condition}")
                count = cursor.fetchone()[0]
                if count > 0:
                    total_remaining += count
                    details.append(f"{table}: {count}")
                    # Force cleanup
                    cursor.execute(f"DELETE FROM {table} WHERE {condition}")
                    self.db_manager.conn.commit()

            cleaned = total_remaining == 0

            self._log_test(
                category,
                "Final Database Integrity",
                cleaned,
                "Database clean - no test data remaining" if cleaned else f"Cleaned {total_remaining} remaining entries: {', '.join(details)}"
            )
            cursor.close()

        except Exception as e:
            self._log_test(category, "Final Database Integrity", False, f"Error: {e}")


def format_results_for_discord(summary: Dict) -> List[str]:
    """
    Format test results for Discord DM (respects 2000 char limit).

    Args:
        summary: Test results summary

    Returns:
        List of message strings to send
    """
    messages = []

    # Header
    header = f"""**Bot Test Suite Results**
Total Tests: {summary['total']}
Passed: {summary['passed']} ✅
Failed: {summary['failed']} ❌
Pass Rate: {summary['pass_rate']:.1f}%

{'='*40}
"""
    messages.append(header)

    # Group results by category
    categories = {}
    for result in summary["results"]:
        category = result["category"]
        if category not in categories:
            categories[category] = []
        categories[category].append(result)

    # Format each category
    current_message = ""
    for category, tests in categories.items():
        category_text = f"\n**{category}**\n"

        for test in tests:
            test_line = f"{test['emoji']} {test['test']}\n"
            if test['details']:
                test_line += f"   ↳ {test['details']}\n"

            # Check if adding this would exceed limit
            if len(current_message) + len(category_text) + len(test_line) > 1900:
                messages.append(current_message)
                current_message = category_text + test_line
            else:
                if category_text not in current_message:
                    current_message += category_text
                current_message += test_line

    # Add remaining content
    if current_message:
        messages.append(current_message)

    return messages


async def run_tests_for_guild(bot, guild_id: int, guild_name: str) -> Dict:
    """
    Run all tests for a specific guild.

    Args:
        bot: Discord bot instance
        guild_id: Guild ID to test
        guild_name: Guild name

    Returns:
        Test results summary
    """
    suite = BotTestSuite(bot, guild_id, guild_name)
    return await suite.run_all_tests()
